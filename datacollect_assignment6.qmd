---
title: "Assignment6"
id: datacollect
format: html
---

1.  **Analyze: Biden-Xi summit data**

```{r results='hide', message=FALSE, warning=FALSE, echo=FALSE, fig.show='hide'}
library(quanteda)
library(quanteda.textmodels)
library(quanteda.textplots)
library(readr)
library(ggplot2)

# Twitter data about President Biden and Xi summit in November 2021
# Do some background search/study on the event

summit <- read_csv("https://raw.githubusercontent.com/datageneration/datamethods/master/textanalytics/summit_11162021.csv")

# Instead of View() for reproducible scripts, use head() or glimpse()
head(summit, 10)

sum_twt <- summit$text

# Tokenize the text
toks <- tokens(sum_twt)
print(class(toks))

# Create document-feature matrix
sumtwtdfm <- dfm(toks)

# Latent Semantic Analysis 
# Reference: https://quanteda.io/reference/textmodel_lsa.html
# NOTE: The 'margin' parameter has been deprecated in recent versions
# Use textmodel_lsa() without margin parameter for standard LSA
sum_lsa <- textmodel_lsa(sumtwtdfm, nd = 4)
summary(sum_lsa)
head(sum_lsa$docs)
print(class(sum_lsa))

# Intrepretation of LSA results
# LSA reduced your high-dimensional space (63,972 features) into 
# 4 latent semantic dimensions while preserving most of the information.

# What Each Column Represents:
# **Column 1-4**: The 4 latent semantic dimensions
# **Each row** (text1-text6): A document's position in semantic space
# **Cell values**: The document's score/loading on that dimension

### Interpretation:
# | Aspect | Meaning |
#  |--------|---------|
#  | **Large positive values** | Document strongly represents that semantic concept |
#  | **Large negative values** | Document's content opposes that semantic concept |
#  | **Values near zero** | Document barely relates to that dimension |
# | **Text2 row** | Very small values overall = document is semantically "neutral" or sparse |
#  | **Text4 row** | Larger values = rich semantic content across dimensions |


# 

# ============================================================================
# HASHTAG ANALYSIS
# ============================================================================

# Create DFM with punctuation removed
tweet_dfm <- tokens(sum_twt, remove_punct = TRUE) |>
  dfm()
head(tweet_dfm)

# Select hashtags (pattern starting with #)
tag_dfm <- dfm_select(tweet_dfm, pattern = "#*")
toptag <- names(topfeatures(tag_dfm, 50))
head(toptag, 10)

# Feature co-occurrence matrix for hashtags
tag_fcm <- fcm(tag_dfm)
head(tag_fcm)

# Select top hashtags for visualization
# Updated: fcm_select() is still valid but ensure compatibility
topgat_fcm <- fcm_select(tag_fcm, pattern = toptag)

# Create network plot with improved parameters
textplot_network(
  topgat_fcm, 
  min_freq = 50, 
  edge_alpha = 0.8, 
  edge_size = 1,
  vertex_size = 3
)

# ============================================================================
# USER MENTION ANALYSIS
# ============================================================================

# Select user mentions (pattern starting with @)
user_dfm <- dfm_select(tweet_dfm, pattern = "@*")
topuser <- names(topfeatures(user_dfm, 50))
head(topuser, 20)

# Feature co-occurrence matrix for users
user_fcm <- fcm(user_dfm)
head(user_fcm, 20)

# Select top users for visualization
user_fcm <- fcm_select(user_fcm, pattern = topuser)

# Create network plot for user mentions
textplot_network(
  user_fcm, 
  min_freq = 20, 
  edge_color = "firebrick", 
  edge_alpha = 0.8, 
  edge_size = 1,
  vertex_size = 3
)

# ============================================================================
# ADDITIONAL ANALYSIS OPTIONS (for reference)
# ============================================================================

# Optional: Keyword frequency analysis
top_features <- topfeatures(tweet_dfm, 30)
print(top_features)

library(ggplot2)
library(tidyverse)

# Get top features with frequencies
top_freq <- topfeatures(tweet_dfm, 30)

# Convert to dataframe for ggplot
freq_df <- tibble(
  word = names(top_freq),
  frequency = as.numeric(top_freq)
) %>%
  mutate(word = fct_reorder(word, frequency))
```

```{r}
# Create frequency plot
ggplot(freq_df, aes(x = frequency, y = word, fill = frequency)) +
  geom_col(show.legend = FALSE) +
  scale_fill_gradient(low = "lightblue", high = "darkblue") +
  labs(
    title = "Top 30 Most Frequent Terms",
    subtitle = "Biden-Xi Summit Tweets",
    x = "Frequency",
    y = "Term"
  ) +
  theme_minimal() +
  theme(
    axis.text = element_text(size = 11),
    title = element_text(size = 13, face = "bold")
  )

```

The most frequent terms in Biden-Xi summit tweets are `biden`, `the`, `xi`, `summit` in order of their frequencies.

2.  **Analyze: US presidential inaugural speeches**

```{r results='hide', message=FALSE, warning=FALSE, echo=FALSE}
library(quanteda)
library(quanteda.textmodels)
library(quanteda.textplots)
library(quanteda.textstats)  # Add textstats library at top
library(readr)
library(ggplot2)
library(tidyverse)
# Create DFM from speeches before 1826
dfm_inaug <- corpus_subset(data_corpus_inaugural) |>
  tokens(remove_punct = TRUE) |>
  tokens_remove(stopwords("english")) |>
  dfm() |>
  dfm_trim(min_termfreq = 10, verbose = FALSE)
```

```{r}

# Get top 100 features with frequency statistics
features_dfm_inaug <- textstat_frequency(dfm_inaug, n = 100)

# Sort by frequency (descending)
features_dfm_inaug <- features_dfm_inaug |>
  mutate(feature = reorder(feature, -frequency))

# Plot frequency distribution
ggplot(features_dfm_inaug, aes(x = feature, y = frequency)) +
  geom_point(color = "steelblue", size = 2) +
  geom_line(color = "steelblue", alpha = 0.3, group = 1) +
  labs(
    title = "Top 100 Terms: Historical Inaugural Speeches (1789-present)",
    x = "Term",
    y = "Frequency"
  ) +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5, size = 8),
    plot.title = element_text(face = "bold")
  )
```

The most common terms in historical presidential inaugural speeches (1789-present) are people, government, us, and can over 400 frequencies.

3.  **Any similarities and differences over time and among presidents?**

```{r figure.height=10}
# Create faceted plot

# Create DFM for recent presidents (2000 onwards)
dfm_weight_pres <- corpus_subset(data_corpus_inaugural, Year > 2000) |>
  tokens(remove_punct = TRUE) |>
  tokens_remove(stopwords("english")) |>
  dfm() |>
  dfm_weight(scheme = "prop")

# Get top 10 terms for each president
freq_weight <- textstat_frequency(
  dfm_weight_pres,
  n = 10,
  groups = docvars(corpus_subset(data_corpus_inaugural, Year > 2000), "President")
)

# Create faceted plot
ggplot(
  data = freq_weight,
  aes(x = reorder(feature, frequency), y = frequency)
) +
  geom_col(fill = "steelblue", alpha = 0.8) +
  facet_wrap(~group, scales = "free_x", ncol = 2) +
  coord_flip() +
  labs(
    title = "Top 10 Terms by President",
    subtitle = "Post-2000 Inaugural Speeches (Relative Frequency)",
    x = "Term",
    y = "Relative Frequency"
  ) +
  theme_minimal() +
  theme(
    strip.text = element_text(face = "bold", size = 11),
    plot.title = element_text(face = "bold")
  )


```

The most frequent used terms in Inaugural speeches by Biden and Obama were `us` while the term used by Bush was `freedom` and the term used by Trump was `america`.

4.  **What is word fish?**

    Wordfish??is a Poisson scaling model used to estimate one-dimensional document positions using maximum likelihood. It allows for the plotting of both the estimated positions of words (features) and the positions of documents within a corpus. This model is particularly referenced in quantitative text analysis as a method for mapping documents along a latent dimension, such as political ideology or topic relevance, based solely on word usage. The method was developed by Slapin and Proksch in 2008 and is implemented in the??`quanteda.textmodels`??package, accessible in R through the function??`textmodel_wordfish()`. The model's results can be plotted to visualize the positions of words or documents as determined by the scaling analysis.???
